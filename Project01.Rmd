---
title: "Group Project 1"
subtitle: "Biology 368/664 Bucknell University"
output: pdf_document
authors: Victoria, Sweta and Kai - Group 3
date: 14 Sep 2024
---

This project will require you to develop a tutorial to teach Bucknell students how to use R for graphing and data analysis.

## Loading the packages

First of all, you'll need to load all the packages that might be required for the project.  

```{r Load Libraries, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!require("rstatix")) install.packages("rstatix"); library(rstatix) # contains the pairwise_t_test function
if (!require("UsingR")) install.packages("UsingR"); library(UsingR) # For the simple.eda function
if (!require("cowplot")) install.packages("cowplot"); library(cowplot) # For clean graphs
if (!require("readxl")) install.packages("readxl"); library(readxl) # For loading excel files
if (!require("conflicted")) install.packages("conflicted"); library(conflicted) # For dealing with conflicts
if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse) # For everything
conflict_prefer_all("dplyr", quiet = TRUE)


```


## Importing the dataset

You'll need to load the dataset using the read_csv or read.csv function. 
For this project, we directly imported the dataset from Tidytuesday in Github.
For other projects, you might want to download the .xls file or other file types and then import it in the R. 
However, you'll have to make sure that your .Rmd project file and the data file is in the same folder or directory in R studio, else the codes will not run.


```{r}
colony <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-11/colony.csv', show_col_types = FALSE)
```

## Colony dataset

The bee colony dataset comes from the USDA (US Department of Agriculture).

### Description of the bee colony data

Every year a significant number of bee colonies are being lost from each of the states in the US. Alarm over honeybee numbers began growing since 2006, when a phenomenon called "Colony collapse disorder" became widely known. This problem, in which the majority of worker bees abandon the colony, has since receded but beekeepers are now faced with more general die-offs linked to disease, pesticide use and habitat loss. 
Since the higher losses has been noticed, agricultural agencies, researchers, and the beekeeping industry have been working together to understand why and develop best management practices to reduce their losses.
A survey is done annually to highlight the continuing high rates of honey bee colony loss in each of the states.

We chose this dataset to visualize and analyze the information regarding hone bee colonies and their losses.

This dataset provides information on honey bee colonies in terms of number of colonies (colony_n), maximum (colony_max),lost (colony_lost), percent lost (colony_lost_pct), added (colony_added), renovated (colony_reno), and percent renovated (colony_reno_pct), as well as colonies lost with Colony Collapse Disorder symptoms with both over and less than five colonies. 


## Data Exploration

You'll need to use the str() function to check out the structure of the dataset. It will tell you about the different variables, their data type, and the number of observations from each of the variables. 

Before you proceed with analyzing the dataset, it is necessary to make sure that it is complete and that you understand what each variable (column) means. Most of the times, You may need to refer to the paper where it is imported from. 

You'll need to look at each variable (column) and determine if it is in the correct data type. The str() function shows the data types of different variables considered by R, while the summary will show what information the variables actually contain.

Some of the variables might need conversion to a different data type.


```{r}
str(colony)
```

```{r}
summary(colony)
```

In this case, R has considered "year" as numeric data type (num), and "months" and "sate' as character (chr). 
Since all of them are categorical variables as shown by the summary, they need to be converted as factors using the as.factor() function.

In cases, where the data needed to be converted to a different data type other than factor, for example into character, then we could use the as.character() function. It works similarly for converting other data types.

### Conversion of the data type of variables

```{r}
colony$year <- as.factor(colony$year)
colony$months <- as.factor(colony$months)
colony$state <- as.factor(colony$state)

str(colony)
```

We check the structure of the dataset again to confirm that the transformation has worked.

```{r}
head(colony)
tail(colony)
```

Using the head() and tail() functions show you the top 6 and bottom 6 observations of your dataset. This is a good way to know if the observations for each of the variables are arranged in order. Because sometimes, you might want to rearrange the order of the levels of the variables for easier analysis.

In this case, the observations seem to have been arranged in chronological manner. For example, the observations for the column "year" starts with 2015 and ends with 2021. It means that there mus be observations of year 2016 to 2020 in an ascending order between 2015 and 2021.
Similarly, the "state" variable is arranged in an alphabetically order.

Overall, it looks like for for each states in the US, the bee colony data has been recorded each year from 2015 to 2021 for each month range (Jan-Mar, Apr-June, July-Sept, Oct-Dec).

This kind of arrangement can be pretty beneficial in most cases.


```{r}
nrow(colony)
```

```{r}
ncol(colony)
```

Checking the number of rows and columns is just another way to check the number of observations you have (you can see them in the structyre too.)

## Creating a clean dataset

Clean dataset refers to creating a new dataset by removing rows or columns that might not be required for data analysis. 

We will only remove the NA's (missing values) for now. It is often necessary do so because they can interfere with data analysis, but honestly, it depends on the context.
For example: Removing the NA's will make sure that the statistical functions run smoothly, it will avoid incorrect results for certain models, like regression, which requires the missing data to be well handled. This might lead to more reliable interpretations.
But at the same time, deleting rows with NA will reduce the sample size which might causing the analysis to lose statistical power. In cases where NA's are not random, removing them might bias the results, especially if missing data is systematically related to certain variables.

Here we are using the pipe function, or |> . This is neat trick that can be used with some r commands (not all of them unfortunately) to basically say, "use the output of this line as the input of the next line". 

```{r}
clean_colony <- colony |> 
  drop_na(colony_n:colony_reno_pct) 
```

The drop_na() function removes rows where any column contains NA's. In this dataset, there were no NA's in the first three columns, so we assigned the function to the remaining columns. 
It can also be modified by choosing specific columns depending on your hypothesis.

```{r}
summary(clean_colony)
```

We can see that after removing NA's, the number of observations decreased from 1222 to 929. 

## Data visualization (graphs!!)



In order to really understand your data and the relationship between factors, it is important to be able to visualize them!

R studio has some built-in plotting functions in the base, but most people seem to agree that using a package called ggplot2 is highly superior. We already loaded this in at the beginning of the tutorial. 

ggplot uses a funny style of code where you can use + to add more layers and edit your graphs. The order you use might be a bit different from what you've done before but you'll get used to it! 

Let's say we are interested in if there's a specific state where many bee colonies are lost. Let's try to make a graph of colonies lost per state using the function ggplot(). I will include a lot of space when I add a new term to the ggplot function so it's easy to read, but when you're actually coding you'd want to keep these compact!


```{r}

ggplot(clean_colony)+  #The name of the data frame you are using goes in the parenthesis
  
  
  
  aes(x=state, y=log((colony_lost)))+ #aes sets the x and y axes. Because there are big differences between states, I am using log to transform the colony lost function and make the states more comparable.
  
  
  
  geom_boxplot() #using geom_*** specifies which type of graph you want 
```

We've got a graph, but it looks pretty scary!!The axis titles are ugly, it's impossible to read the state names, and the grid background is distracting. 

We can fix some of these problems. 

```{r}

ggplot(clean_colony)+  
  aes(x=state, y=log((colony_lost)))+
  geom_boxplot(width = .7)+
  
  
  
  coord_flip()+ #this is an easy way to switch the x and y axis so labels easier to read
  
  
  xlab("State") + #this fixes the labels so they say what we want
  ylab("Colonies Lost")+
  
  
  
  theme_classic(base_size = 9)+ # this removes the annoying grid in the background
  
  
  
  geom_jitter(alpha=.1) #you can choose to show all the data points underlying the boxplot with this function. It's often a good idea to be transparent with your data so this is often a good idea, but in this case I felt it just made the graph look messy, even when I used alpha to make them transparant. 

```

However, this is still pretty hard to read, since there are so many states. While it makes sense initially to arrange them alphabetically, it might be better to range them in order of their medians. We can add use reorder() in the aes command to choose the order of our states. 

```{r}
ggplot(clean_colony)+  
  aes(x=reorder(state,colony_lost), y=log((colony_lost)), fill=state)+ #you can also add in color to make this a bit more lively. This will give a different color for each category
  
  geom_boxplot(width = .7, show.legend=FALSE)+ #however, for this specific graph, having a legend is redundant so we can remove it like this
  
  coord_flip()+ 
  xlab("State") +
  ylab("Colonies Lost")+
  theme_classic(base_size = 9)
```

We can see that there is a range of loss across states, with New England having some of the smallest losses while states like California and North Dakota have a lot. We also have to remember that we used a log transformation on the losses to make them more readable. If we remove this transformation, the amount of loss in California appears all the more dramatic. 


```{r}
ggplot(clean_colony)+  
  aes(x=reorder(state,colony_lost), y=(colony_lost), fill=state)+
  geom_boxplot(width = .7, show.legend=FALSE)+
  coord_flip()+ 
  xlab("State") +
  ylab("Colonies Lost")+
  theme_classic(base_size = 9)
```

However, the number of colonies lost may not be the best metric of which states are actually the most affected, since some states may simply have larger bee populations to start with and thus greater numbers of loss even if a small percentage is hurt. 

To fix this, we can look at the colony_lost_pct, which gives the loss as a percentage. 


Create a boxplot like the ones above with states and percentages lost.You may see that the results vary a lot!


Try graphs both with and without log transformation and feel free to play around with stylizations. 


```{r}

```




When making a graph with ggplot2, it is important that you take a minute to think about what graph type is best for your data. For example, you can technically make a scatterplot using geom_point using the colony lost and state information. 

```{r}
ggplot(clean_colony)+  
  aes(x=reorder(state,colony_lost), y=log((colony_lost)))+ 
  geom_point(show.legend=FALSE)+ 
  coord_flip()+ 
  xlab("State") +
  ylab("Colonies Lost")+
  theme_classic(base_size = 9)
```

However, it's pretty nightmarish to try to read, and though we can still get a general idea of the trends it overall gives less information than the boxplot. If you're not sure about what kind of plot is best for your data, think about whether you have categorical or numerical variables, and if they are continuous. There are lots of cheatsheets and articles available online if you're not sure.

A scatterplot is most appropriate for two continuous (numerical) variables. For example, we could plot colonies lost vs colonies added, splitting by year.

The RColorBrewer package is just a nice way to get specific color palettes if you want your graphs to look prettier.

You can view all the palettes here: 
https://r-graph-gallery.com/38-rcolorbrewers-palettes.html



```{r}

if (!require("RColorBrewer")) install.packages("RColorBrewer"); library(RColorBrewer)

ggplot(clean_colony)+  
  aes(x=log(colony_lost), y=log(colony_added), color=year)+ 
  geom_point()+ 
  xlab("Colonies Lost") +
  ylab("Colonies Added")+
  theme_classic()+
  scale_color_brewer(palette="OrRd")
```

From this, we can see that the relationship is mostly linear--as more colonies are lost, more are added as well. However, there is also a pretty large "hump" hanging down below the 5.0 y-axis, suggesting there are places with pretty significant colony loss that don't add in new colonies. We can also see a small trend with the points from 2021 tending to be higher, suggesting that perhaps beekeepers are mobilizing to add more new colonies. 


We can further test this trend by adding linear models for each year using geom_smooth. (Of course, you would want to be sure using a linear model is appropriate. We will assume it is for here.) You will want to include the argument "method="lm"" to tell it to use a linear model. Otherwise, it'll try to fit the with kinda funky, not-so-great to interpret curvy line. Se=false gets rid of messy error bars, though you may want these in some cases.

```{r}
ggplot(clean_colony)+  
  aes(x=log(colony_lost), y=log(colony_added), color=year)+ 
  geom_point()+ 
  xlab("Colonies Lost") +
  ylab("Colonies Added")+
  theme_classic()+
  scale_color_brewer(palette="OrRd")+
  geom_smooth(method="lm",se=FALSE)
```

The y-intercept for 2021 is highest, suggesting a greater number of colonies is being added compared to those being lost. 


Now try to make a scatterplot comparing colony percentage lost to percentage renovated and split it by year. 


```{r}

```



##Hypothesis Testing


Although the graphs we made are helpful to develop some initial trends and see our data, it's impossible to tell whether any trends we see are actually significant or could be due to chance. 

Since we are looking at data collected from the real world and not an experiment, we can't really suggest causation, but we can test for correlations between factors. 


An easy way to test correlation is to use a linear model. We can start by looking at if the percentage lost varies by year


```{r}

lmbee<-glm(colony_lost_pct~year, data=clean_colony)
summary(lmbee)

```

Before continuing with interpretation, however, we should test that using a linear model is appropriate for this data and fits well. To do this, we can look at the qqplot, which shows the distance of the residuals from a common line. 

```{r}
plot(lmbee)
```
In a well-fitting model, the points should fall on the plotted line. Some variation is to be expected, though, it may be helpful to do a log transformation on the loss percentage variable.

```{r}
lmbeelog<-glm(log(colony_lost_pct)~year, data=clean_colony)
summary(lmbeelog)
plot(lmbeelog)
```

We can see that this improves the deviating upper tail a bit. 

Now that we know that using a linear model is OK, we can look to interpret the model further. While the summary function spits out some p-values, these can be inaccurate as they don't account for error from repeating tests over many categories. 

Doing an Anova can provide more accurate p-values.

```{r}
anova(lmbeelog)
```
The significant p-value tells us that at least one group is significantly different from Arizona. We can follow this up with a tukey test, which will tell us exactly which groups are different. 

```{r}
if (!require("multcomp")) install.packages("multcomp"); library(multcomp)
tukey<-glht(lmbeelog, linfct=mcp(year="Tukey"))
summary(tukey)
```

Unfortunately, it appears that when p-values are adjusted to account for multiple comparisions, none of the years are significantly different. However, we can also improve the linear model by adding addition explanatory variables, such as the state and months. 



```{r}
lmbeenew<-glm(log(colony_lost_pct)~year + state + months, data=clean_colony)
summary(lmbeenew)
plot(lmbeenew)
```


The qqplot seems ok.

```{r}
anova(lmbeenew)
tukey2<-glht(lmbeenew, linfct=mcp(year="Tukey"))
summary(tukey2)

```


Using a more complex model that accounts for state and month, there appears to be a significant difference between 2020 and 2016. 




Now, try to use a linear model and anova to test whether the max colony amount correlates with year. You may want to include state as an additional explanatory variable. 


Be sure to set your cutoff p-value before running the tests. R automatically uses 0.05, but you may interested in a different cutoff. 


```{r}

```

If significance is found, follow up with a tukey test.

```{r}

```


## Introduction

Begin by introducing yourself to your group. 
Then discuss the biggest challenge that you have faced during the first three weeks of this course.
Determine if there are any common threads in these challenges and start to think about objectives for the tutorial.

## Target Audience

Discuss with your group the target audience for the tutorial. 
Examples could be a new student in Biology 364/664, a student in one of the new core Biology classes (201, 202, 203, or 204), a student in another 300-level Biology course (not 364), or a new student in one of the Bucknell research groups. 

Edit the README.md file in your group's repository to reflect your plan.

## Objectives

After deciding on the target audience for your tutorial determine 2 to 3 overall objectives for your tutorial (one per member of your group). 
These should be high-level objectives that are important skills for your target audience.
Check with Prof. Field to see if they are appropriate and then add them to your README.md file.

Identify at least 2 goals within each objective and add them to your README.md file.
These should be goals that someone who is working through the tutorial can self-asses.
For example, "to demostrate that you can test a hypothesis using a statistical model, the student should use a T test, linear model, or other test and interpret the p value appropriately."

## Grading

Each group will be expected to complete the following tasks to earn 85% of the points available for this assignment (21/25).

- Identify and obtain suitable dataset
- Use a Github repository and version control to collaborate on the project
  + Every member of the group should participate in editing the repo
- Spend 4-6 hours preparing, coding, and testing tutorial
  + Data exploration
  + Data visualization
  + Hypothesis testing
- Present tutorial in class
- Provide public archive suitable for sharing to students/faculty

Tutorials from previous classes can be viewed at our public github site: https://github.com/Bucknell-Biol364

Each group should use an *Acknowledgements* section to document the participation of each member and the collaboration within and between groups.

Additional credit will be awarded for providing assistance to other groups or for the development of a tutorial that goes beyond the minimal expectations listed above.
You will have the opportunity to provide feedback to another group after the initial deadline (like for Homework 02).


