---
title: "Group Project 1"
subtitle: "Biology 368/664 Bucknell University"
output: github_document
authors: Victoria, Sweta and Kai - Group 3
date: 14 Sep 2024
---

Hello! This is a tutorial meant to help you do some basic data analysis and make graphs using R Studio. We assume that you've already had R installed and updated and you may have had some experience in the past with coding in R. If not, there are many resources online to get you up to speed!

We'll be giving a lot of examples from a dataset of bee colony loss across the United States, a recent and pressing ecological and agricultural problem. We've also included some areas for you to put your own understanding into practice and try to write your own code. 

There are a couple of objectives for you to follow along with in this assignment: 

1. To be able to explore data within a dataset based on the data type 
      to be able to convert variables into different type
      to be able to identify outliers, messy data, or non-normal distributions
      to be able to log transform the variables to visualize them better
      
2. To use filter to create a subset of a variable appropriate to your analysis question

3. To create appropriate and clear graphs to visualize data in both categorical and numeric variables represented in the hypothesis using ggplot2
    to be able to create a scatterplot and box and whisker plot
    to be able to edit labels and change the style to fit their preferences
    to able to interpret the plot 
    
4. To use an anova and linear model to help us interpret variables in our provided hypothesis
 to be able to create a linear model
 to be able to do follow up anova and tukey tests
 to be able to interpret the results from these tests


## Loading the packages

First of all, you'll need to load all the packages that might be required for the project that will allow you to use certain functions.  

```{r Load Libraries, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!require("rstatix")) install.packages("rstatix"); library(rstatix) # contains the pairwise_t_test function
if (!require("stats")) install.packages("stats"); library(rstatix) # used to run Shapiro test
if (!require("UsingR")) install.packages("UsingR"); library(UsingR) # For the simple.eda function
if (!require("cowplot")) install.packages("cowplot"); library(cowplot) # For clean graphs
if (!require("readxl")) install.packages("readxl"); library(readxl) # For loading excel files
if (!require("conflicted")) install.packages("conflicted"); library(conflicted) # For dealing with conflicts
if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse) # For everything
conflict_prefer_all("dplyr", quiet = TRUE)


```


## Importing the dataset

You'll need to load the dataset using the read_csv or read.csv function. 
For this project, we directly imported the dataset from Tidytuesday in Github.
For other projects, you might want to download the .xls file or other file types and then import it in the R. 
However, you'll have to make sure that your .Rmd project file and the data file is in the same folder or directory in R studio, else the codes will not run.


```{r}
colony <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-11/colony.csv', show_col_types = FALSE)
```

## Colony dataset

The bee colony dataset comes from the USDA (US Department of Agriculture).

### Description of the bee colony data

Every year a significant number of bee colonies are being lost from each of the states in the US. Alarm over honeybee numbers began growing since 2006, when a phenomenon called "Colony collapse disorder" became widely known. This problem, in which the majority of worker bees abandon the colony, has since receded but beekeepers are now faced with more general die-offs linked to disease, pesticide use and habitat loss. 
Since the higher losses has been noticed, agricultural agencies, researchers, and the beekeeping industry have been working together to understand why and develop best management practices to reduce their losses.
A survey is done annually to highlight the continuing high rates of honey bee colony loss in each of the states.

This dataset provides information on honey bee colonies in terms of number of colonies (colony_n), maximum (colony_max),lost (colony_lost), percent lost (colony_lost_pct), added (colony_added), renovated (colony_reno), and percent renovated (colony_reno_pct), as well as colonies lost with Colony Collapse Disorder symptoms with both over and less than five colonies. 

We chose this dataset because: 
1. It is a large dataset, so it can be beneficial to make many hypothesis and visualize and analyze the information regarding hone bee colonies and their losses.
2. No background knowledge/information is required to understand the dataset; it's simple and easy to understand just by reading the description provided above (and looking at our variables).
3. It contains both categorical and numerical variables such that we can different graphs and perform different statistical tests accordingly.


## Data Exploration

You'll need to use the str() function to check out the structure of the dataset. It will tell you about the different variables, their data type, and the number of observations from each of the variables. 

Before you proceed with analyzing the dataset, it is necessary to make sure that it is complete and that you understand what each variable (column) means. Most of the times, You may need to refer to the paper where it is imported from. 

You'll need to look at each variable (column) and determine if it is in the correct data type. The str() function shows the data types of different variables considered by R, while the summary will show what information the variables actually contain.

Some of the variables might need conversion to a different data type.


```{r}
str(colony)
```

```{r}
summary(colony)
```

In this case, R has considered "year" as numeric data type (num), and "months" and "sate' as character (chr). 
Since all of them are categorical variables as shown by the summary, they need to be converted as factors using the as.factor() function.

In cases, where the data needed to be converted to a different data type other than factor, for example into character, then we could use the as.character() function. It works similarly for converting other data types.

### Conversion of the data type of variables

```{r}
colony$year <- as.factor(colony$year)
colony$months <- as.factor(colony$months)
colony$state <- as.factor(colony$state)

str(colony)
```

We check the structure of the dataset again to confirm that the transformation has worked.

```{r}
head(colony)
tail(colony)
```

Using the head() and tail() functions show you the top 6 and bottom 6 observations of your dataset. This is a good way to know if the observations for each of the variables are arranged in order. Because sometimes, you might want to rearrange the order of the levels of the variables for easier analysis.

In this case, the observations seem to have been arranged in chronological manner. For example, the observations for the column "year" starts with 2015 and ends with 2021. It means that there mus be observations of year 2016 to 2020 in an ascending order between 2015 and 2021.
Similarly, the "state" variable is arranged in an alphabetically order.

Overall, it looks like for for each states in the US, the bee colony data has been recorded each year from 2015 to 2021 for each month range (Jan-Mar, Apr-June, July-Sept, Oct-Dec).

This kind of arrangement can be pretty beneficial in most cases.


```{r}
nrow(colony)
```

```{r}
ncol(colony)
```

Checking the number of rows and columns is just another way to check the number of observations you have (you can see them in the structure too.)

## Creating a clean dataset

Clean dataset refers to creating a new dataset by removing rows or columns that might not be required for data analysis. 

We will only remove the NA's (missing values) for now. It is often necessary do so because they can interfere with data analysis, but honestly, it depends on the context.

For example: Removing the NA's will make sure that the statistical functions run smoothly, it will avoid incorrect results for certain models, like regression, which requires the missing data to be well handled. This might lead to more reliable interpretations.
But at the same time, deleting rows with NA will reduce the sample size which might causing the analysis to lose statistical power. In cases where NA's are not random, removing them might bias the results, especially if missing data is systematically related to certain variables.

Here we are using the pipe function, or |> . This is neat trick that can be used with some r commands (not all of them unfortunately) to basically say, "use the output of this line as the input of the next line". 

```{r}
clean_colony <- colony |> 
  drop_na(colony_n:colony_reno_pct) # the : sign is used to select the all the columns from colony_n to the last column colony_reno_pct. Only use that sign if you want to drop NA's of all the columns existing between them.
```

The drop_na() function removes rows where any column contains NA's. In this dataset, there were no NA's in the first three columns, so we assigned the function to the remaining columns. 
It can also be modified by choosing specific columns depending on your hypothesis.

```{r}
summary(clean_colony)
```

We can see that after removing NA's, the number of observations decreased from 1222 to 929. 


#Filtering Data

Besides dropping NA's, using the filter function can be another strategy to create clean datasets and eliminate irrelevant information. 

After all of that complicated and scary data exploration, you might still have questions about the data set. filter() is a great code you can use to filter out the data to create a subset of variables appropriate to analyze your remaining questions. 

Let's assume we want to analyze data for states with high colony losses in the first quarter of 2015 (January-March) where the colony loss percentage (colony_lost_pct) is greater than 20%, because we want to find out which state is responsible for bee colony loss. Here's how you would go about filtering this data.
 
#1 understand your variables. 

Here we can use str() again to check out which variables we need to filter for our question. 


```{r}
str(colony)

```

We will focus on the months, state, and colony_lost_pct columns for filtering.

#2 Applying filter

We need to filter the data for:
  1) Time period between January- March of 2015
  2) State with colony loss greater than 20%
  
```{r}
# Filter for January-March 2015 and colony loss percentage greater than 20%
filtered_colony2015 = colony[(colony[["year"]] == 2015) & 
                        (colony[["months"]] == "January-March") & 
                        (colony[["colony_lost_pct"]] > 20), ]

# Display the filtered dataset
print(filtered_colony2015)

```
This is a good way to filter data set for complex questions as we stated above. Or, if you have simpler questions such as those below.


#Example 1

Filter Rows by a Single Condition If you want to filter the data for records where the state is "California"
```{r}
california_data <- filter(colony, state == "California")

```

simple right? 


#Example 2

Filter Rows by Multiple Conditions If you want to filter rows where the colony loss percentage is greater than 10% and the state is "Florida"
```{r}
Florida_high_loss <- filter(colony, state == "Florida" & colony_lost_pct > 10)
```

The filter() function is an essential tool when analyzing data in R. You can use it to isolate specific subsets of your data that are relevant to your analysis by defining conditions based on the values in your dataset's columns. Hope this demonstration of the filter function helped you. 


Test it out yourself by using the filter() function to create a data frame with only rows from the state of Pennslyvania.

```{r}

```


## Data visualization (graphs!!)

We have many numeric variables in our dataset. One easy way to visualize the numeric variables independently is using the "simple.eda" function. It means Simple Exploratory Data Analysis. 
This function generates three graphs (Histogram, boxplot and Normal Q-Q plot) for each of the variables, and allows us to tell whether they are normally distributed or not.

```{r}
simple.eda(clean_colony$colony_n)
simple.eda(clean_colony$colony_max)
simple.eda(clean_colony$colony_lost)
simple.eda(clean_colony$colony_lost_pct)

```

The graphs look pretty ugly for each of the four variables. 
The graphs for the colonies lost (colony_lost) is better than the others, but it still is very skewed at the right end. These show that the data is not normally distributed at all.

You should try the simple.eda for other remaining numeric variables and find out whether they are normally distributed or not:

```{r}


```

Well, one other way to check whether the data is normally distributed or not is by conducting a Shapiro test. It's a good way to test for normality before conducting other statistical tests.

```{r}
shapiro.test(clean_colony$colony_n)
```

The p-value is below 0.05, which suggests that the data is nor normally distributed. Generally, a low  p-value tells you it is unlikely the results seen are due to chance. In this case, the hypothesis tested is that the data is NOT normal, therefore a p-value below 0.05 means there is a 95% confidence that the data is not normal. 

```{r}
shapiro.test(clean_colony$colony_lost)
```

It's the same for colonies lost too.

We want you to check the same tests for other numerical variables too.

```{r}

```

# Log transformation of the data

Since the graphs generated from the simlple.eda were very boring and not justifiable enough. We can log transform them for better visualization and analysis. We use the log10 function for that.

We can use the mutate() function for that. The mutate() function exists to compute transformations of variables in a data frame.

```{r}
clean_colony <- clean_colony |> 
  mutate(clean_colony, log.colony_n = log10(colony_n), log.colony_max = log10(colony_max)) #using the mutate function to incorporate the log value to the number of colonies and the maximum colonies to transform them
summary(clean_colony) #checking the summary after the log transformation allows you to observe/view the "log.colon_n" and "log.colony_max" as new columns in the dataframe.
        
```

You could use the variables one by one in the above code, or all the  variables that needs to be log transformed together in a single code using the coma(,) to separate the variables like shown above. 

For practice, you can either add the formulas for remaining variables in the chunk provided above or just write them again below:


```{r}

```

Make sure to check the summary again to ensure that the new log transformations are successful.

Now that we have log transformed the data, we want you to conduct the exploratory analysis again to see if you notice any difference after the log transformation.
We have done one for you using the colony_n variable.

```{r}
simple.eda(clean_colony$log.colony_n)
simple.eda(clean_colony$log.colony_max)

```

As you can see that the graphs have incredibly improved after the log transformation for both the variables. 
We want to check using the Shapiro test again to make sure the transformation was successful.

```{r}
shapiro.test(clean_colony$log.colony_n)
shapiro.test(clean_colony$log.colony_max)

```

Well, surprisingly, the Shapiro test still yields undesired result.The p-value must have been greater than 0.05 to provide 95% confidence that the data is normally distributed. 
Since it's a very large dataset and we have performed other statistical tests down below, this does not really matter. 

However, we still want you to see if the log transformations helped the other numeric variables. 

```{r}




```


Try running Shapiro tests for them too: 

```{r}



```


Do you see any significant difference between the old and the new graphs? Do the new ones look more normally distributed?  What about the p-value in the Shapiro test?


### Visualizing relationship between two or more variables

In order to really understand your data and the relationship between factors, it is important to be able to visualize them!

R studio has some built-in plotting functions in the base, but most people seem to agree that using a package called ggplot2 is highly superior. We already loaded this in at the beginning of the tutorial. 

ggplot uses a funny style of code where you can use + to add more layers and edit your graphs. The order you use might be a bit different from what you've done before but you'll get used to it! 

Let's say we are interested in if there's a specific state where many bee colonies are lost. Let's try to make a graph of colonies lost per state using the function ggplot(). I will include a lot of space when I add a new term to the ggplot function so it's easy to read, but when you're actually coding you'd want to keep these compact!


```{r}

ggplot(clean_colony)+  #The name of the data frame you are using goes in the parenthesis
  
  
  
  aes(x=state, y=log((colony_lost)))+ #aes sets the x and y axes. Because there are big differences between states, I am using log to transform the colony lost function and make the states more comparable.
  
  
  
  geom_boxplot() #using geom_*** specifies which type of graph you want 
```

We've got a graph, but it looks pretty scary!!The axis titles are ugly, it's impossible to read the state names, and the grid background is distracting. 

We can fix some of these problems. 

```{r}

ggplot(clean_colony)+  
  aes(x=state, y=log((colony_lost)))+
  geom_boxplot(width = .7)+
  
  
  
  coord_flip()+ #this is an easy way to switch the x and y axis so labels easier to read
  
  
  xlab("State") + #this fixes the labels so they say what we want
  ylab("Colonies Lost/Year")+
  
  
  
  theme_classic(base_size = 9)+ # this removes the annoying grid in the background
  
  
  
  geom_jitter(alpha=.1) #you can choose to show all the data points underlying the boxplot with this function. It's often a good idea to be transparent with your data so this is often a good idea, but in this case I felt it just made the graph look messy, even when I used alpha to make them transparant. 

```

However, this is still pretty hard to read, since there are so many states. While it makes sense initially to arrange them alphabetically, it might be better to range them in order of their medians. We can add use reorder() in the aes command to choose the order of our states. 

```{r}
ggplot(clean_colony)+  
  aes(x=reorder(state,colony_lost), y=log((colony_lost)), fill=state)+ #you can also add in color to make this a bit more lively. This will give a different color for each category
  
  geom_boxplot(width = .7, show.legend=FALSE)+ #however, for this specific graph, having a legend is redundant so we can remove it like this
  
  coord_flip()+ 
  xlab("State") +
  ylab("Colonies Lost")+
  theme_classic(base_size = 9)
```

We can see that there is a range of loss across states, with New England having some of the smallest losses while states like California and North Dakota have a lot. We also have to remember that we used a log transformation on the losses to make them more readable. If we remove this transformation, the amount of loss in California appears all the more dramatic. 


```{r}
ggplot(clean_colony)+  
  aes(x=reorder(state,colony_lost), y=(colony_lost), fill=state)+
  geom_boxplot(width = .7, show.legend=FALSE)+
  coord_flip()+ 
  xlab("State") +
  ylab("Colonies Lost")+
  theme_classic(base_size = 9)
```

However, the number of colonies lost may not be the best metric of which states are actually the most affected, since some states may simply have larger bee populations to start with and thus greater numbers of loss even if a small percentage is hurt. 

To fix this, we can look at the colony_lost_pct, which gives the loss as a percentage. 


Create a boxplot like the ones above with states and percentages lost.You may see that the results vary a lot!


Try graphs both with and without log transformation and feel free to play around with stylizations. 


```{r}

```




When making a graph with ggplot2, it is important that you take a minute to think about what graph type is best for your data. For example, you can technically make a scatterplot using geom_point using the colony lost and state information. 

```{r}
ggplot(clean_colony)+  
  aes(x=reorder(state,colony_lost), y=log((colony_lost)))+ 
  geom_point(show.legend=FALSE)+ 
  coord_flip()+ 
  xlab("State") +
  ylab("Colonies Lost")+
  theme_classic(base_size = 9)
```

However, it's pretty nightmarish to try to read, and though we can still get a general idea of the trends it overall gives less information than the boxplot. If you're not sure about what kind of plot is best for your data, think about whether you have categorical or numerical variables, and if they are continuous. There are lots of cheatsheets and articles available online if you're not sure.

A scatterplot is most appropriate for two continuous (numerical) variables. For example, we could plot colonies lost vs colonies added, splitting by year.

The RColorBrewer package is just a nice way to get specific color palettes if you want your graphs to look prettier.

You can view all the palettes here: 
https://r-graph-gallery.com/38-rcolorbrewers-palettes.html



```{r}

if (!require("RColorBrewer")) install.packages("RColorBrewer"); library(RColorBrewer)

ggplot(clean_colony)+  # the general formatting is very similar to making the barplot
  aes(x=log(colony_lost), y=log(colony_added), color=year)+ 
  geom_point()+ #use geom_point to create a scatterplot
  xlab("Colonies Lost") +
  ylab("Colonies Added")+
  theme_classic()+
  scale_color_brewer(palette="OrRd") #this calls in colorbrewer and tells it I want to use the OrRd palatte
```

From this, we can see that the relationship is mostly linear--as more colonies are lost, more are added as well. However, there is also a pretty large "hump" hanging down below the 5.0 y-axis, suggesting there are places with pretty significant colony loss that don't add in new colonies. We can also see a small trend with the points from 2021 tending to be higher, suggesting that perhaps beekeepers are mobilizing to add more new colonies. 


We can further test this trend by adding linear models for each year using geom_smooth. (Of course, you would want to be sure using a linear model is appropriate. We will assume it is for here.) You will want to include the argument "method="lm"" to tell it to use a linear model. Otherwise, it'll try to fit the with kinda funky, not-so-great to interpret curvy line. Se=false gets rid of messy error bars, though you may want these in some cases to show how good the fit is.

```{r}
ggplot(clean_colony)+  
  aes(x=log(colony_lost), y=log(colony_added), color=year)+ 
  geom_point()+ 
  xlab("Colonies Lost") +
  ylab("Colonies Added")+
  theme_classic()+
  scale_color_brewer(palette="OrRd")+
  geom_smooth(method="lm",se=FALSE)
```

The y-intercept for 2021 is highest, suggesting a greater number of colonies is being added compared to those being lost. 


Now try to make a scatterplot comparing colony percentage lost to percentage renovated and split it by year. 


```{r}

```



##Hypothesis Testing


Although the graphs we made are helpful to develop some initial trends and see our data, it's impossible to tell whether any trends we see are actually significant or could be due to chance. 

Since we are looking at data collected from the real world and not an experiment, we can't really suggest causation, but we can test for correlations between factors. 


An easy way to test correlation is to use a linear model. We can start by looking at if the percentage lost varies by year. 

We hypothesize that the percentage lost will increase by year, with newer years like 2021 being significantly higher than years like 2016. 

```{r}

lmbee<-glm(colony_lost_pct~year, data=clean_colony)
summary(lmbee)

```

Before continuing with interpretation, however, we should test that using a linear model is appropriate for this data and fits well. To do this, we can look at the qqplot, which shows the distance of the residuals from a common line. 

```{r}
plot(lmbee)
```
In a well-fitting model, the points should fall on the plotted line. Some variation is to be expected, though, it may be helpful to do a log transformation on the loss percentage variable.

```{r}
lmbeelog<-glm(log(colony_lost_pct)~year, data=clean_colony)
summary(lmbeelog)
plot(lmbeelog)
```

We can see that this improves the deviating upper tail a bit. 

Now that we know that using a linear model is OK, we can look to interpret the model further. While the summary function spits out some p-values, these can be inaccurate as they don't account for error from repeating tests over many categories. 

Doing an Anova can provide more accurate p-values. In this case, the p-value represents the chance that null hypothesis is true. In the anova, the null hypothesis is that there is NOT a difference between the distribution of two groups. So, when we have a low p-value, it means that there is a very low chance that the two groups have the same distribution and thus they are different!

```{r}
anova(lmbeelog)
```
The significant p-value tells us that at least one group is significantly different from Arizona. We can follow this up with a tukey test, which will tell us exactly which groups are different. 

```{r}
if (!require("multcomp")) install.packages("multcomp"); library(multcomp)
tukey<-glht(lmbeelog, linfct=mcp(year="Tukey"))
summary(tukey)
```

Unfortunately, it appears that when p-values are adjusted to account for multiple comparisons, none of the years are significantly different. However, we can also improve the linear model by adding addition explanatory variables, such as the state and months. 

Different states could have difference beekeeping policies, climates, and environmental stressors, meaning that state could effect the percentages of bees lost. Similarly, different months have different climates, which could also be a factor in colony loss. 



```{r}
lmbeenew<-glm(log(colony_lost_pct)~year + state + months, data=clean_colony)
summary(lmbeenew)
plot(lmbeenew)
```


The qqplot seems ok in distribution.

```{r}
anova(lmbeenew)
tukey2<-glht(lmbeenew, linfct=mcp(year="Tukey"))
summary(tukey2)

```


Using a more complex model that accounts for state and month, there appears to be a significant difference between 2020 and 2016. 

If we combine our early filtering skills with our plotting skills, we can see if 2020 is actually higher liked we hyposizd earlier.

```{r}
twoyears<-filter(clean_colony, year=="2016" | year=="2020")
ggplot(twoyears)+  
  aes(x=year, y=log(colony_lost))+ 
  geom_boxplot()+
  xlab("State") +
  ylab("Colonies Lost")+
  theme_classic()

```

In fact, our earlier hyopothesis is not supported and it appears the percentage of colonies lost is actually LOWER in 2020.


Now, try to use a linear model and anova to test whether the max colony amount correlates with year. You may want to include state as an additional explanatory variable. 


Be sure to set your cutoff p-value before running the tests. R automatically uses 0.05, but you may interested in a different cutoff. 


```{r}

```

If significance is found, follow up with a tukey test.


Thank you for following the tutorial! I hoped you learned something new!

*Acknowledgements*
 
Kai worked on the data filtering section, Sweta wrote the initial data analysis, NA removal, simple.eda, log transformation and Shapiro tests, and Victoria did the visualization and linear modeling sections. 


